{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:34.285506Z",
     "iopub.status.busy": "2024-02-28T13:01:34.285043Z",
     "iopub.status.idle": "2024-02-28T13:01:34.298377Z",
     "shell.execute_reply": "2024-02-28T13:01:34.297081Z",
     "shell.execute_reply.started": "2024-02-28T13:01:34.285472Z"
    }
   },
   "outputs": [],
   "source": [
    "SYS_INPUT_DIR = '/kaggle/input/pii-detection-removal-from-educational-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:39.363452Z",
     "iopub.status.busy": "2024-02-28T13:01:39.362995Z",
     "iopub.status.idle": "2024-02-28T13:01:39.817015Z",
     "shell.execute_reply": "2024-02-28T13:01:39.816079Z",
     "shell.execute_reply.started": "2024-02-28T13:01:39.363418Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:44.533780Z",
     "iopub.status.busy": "2024-02-28T13:01:44.532899Z",
     "iopub.status.idle": "2024-02-28T13:01:47.981952Z",
     "shell.execute_reply": "2024-02-28T13:01:47.980702Z",
     "shell.execute_reply.started": "2024-02-28T13:01:44.533741Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train_json = json.load(open(os.path.join(SYS_INPUT_DIR, \"train.json\")))\n",
    "data = pd.json_normalize(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:50.236543Z",
     "iopub.status.busy": "2024-02-28T13:01:50.236028Z",
     "iopub.status.idle": "2024-02-28T13:01:50.257211Z",
     "shell.execute_reply": "2024-02-28T13:01:50.256133Z",
     "shell.execute_reply.started": "2024-02-28T13:01:50.236496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check train has one row per document\n",
    "assert data['document'].nunique() == data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:53.061171Z",
     "iopub.status.busy": "2024-02-28T13:01:53.059697Z",
     "iopub.status.idle": "2024-02-28T13:01:54.874698Z",
     "shell.execute_reply": "2024-02-28T13:01:54.873274Z",
     "shell.execute_reply.started": "2024-02-28T13:01:53.061115Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val_test = train_test_split(data, test_size=0.4, random_state=42)\n",
    "val, test = train_test_split(val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:56.139536Z",
     "iopub.status.busy": "2024-02-28T13:01:56.139109Z",
     "iopub.status.idle": "2024-02-28T13:01:56.146389Z",
     "shell.execute_reply": "2024-02-28T13:01:56.144696Z",
     "shell.execute_reply.started": "2024-02-28T13:01:56.139504Z"
    }
   },
   "outputs": [],
   "source": [
    "full_ner_labels = [\n",
    "    'B-NAME_STUDENT', 'I-NAME_STUDENT',\n",
    "    'B-URL_PERSONAL', 'I-URL_PERSONAL',\n",
    "    'B-ID_NUM', 'I-ID_NUM',\n",
    "    'B-EMAIL', 'I-EMAIL',\n",
    "    'B-STREET_ADDRESS', 'I-STREET_ADDRESS',\n",
    "    'B-PHONE_NUM', 'I-PHONE_NUM',\n",
    "    'B-USERNAME', 'I-USERNAME'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:01:59.309656Z",
     "iopub.status.busy": "2024-02-28T13:01:59.309029Z",
     "iopub.status.idle": "2024-02-28T13:02:01.105165Z",
     "shell.execute_reply": "2024-02-28T13:02:01.103633Z",
     "shell.execute_reply.started": "2024-02-28T13:01:59.309600Z"
    }
   },
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'O':0,\n",
    "    'B-NAME_STUDENT':1, 'I-NAME_STUDENT':2,\n",
    "    'B-URL_PERSONAL':3, 'I-URL_PERSONAL':4,\n",
    "    'B-ID_NUM':5, 'I-ID_NUM':6,\n",
    "    'B-EMAIL':7, 'I-EMAIL':8,\n",
    "    'B-STREET_ADDRESS':9, 'I-STREET_ADDRESS':10,\n",
    "    'B-PHONE_NUM':11, 'I-PHONE_NUM':12,\n",
    "    'B-USERNAME':13, 'I-USERNAME':14\n",
    "}\n",
    "from datasets import Features,ClassLabel,Sequence,Value,Dataset\n",
    "\n",
    "def chunkdf(df):# expects document, tokens and labels\n",
    "    n_tokens = 400\n",
    "    df_chunked = pd.DataFrame(columns=df.columns)\n",
    "    for index,row in df.iterrows():\n",
    "        l_token = [];l_label = [];l_docnum = [];last = 0\n",
    "        n_loop=len(df.tokens[index])//n_tokens+1\n",
    "        for i in range(n_loop-1):\n",
    "            l_token=df.tokens[index][n_tokens*i:n_tokens*(i+1)]\n",
    "            l_label=df.labels[index][n_tokens*i:n_tokens*(i+1)]\n",
    "            l_docnum=str(df.document[index])+'|'+str(i)\n",
    "            sub = {\"document\":l_docnum,\"tokens\":l_token,\"labels\":l_label}\n",
    "            df_chunked = df_chunked._append(sub,ignore_index=True)\n",
    "            last = i+1\n",
    "        #assert last!=0,print(f\"failed for {index}\")\n",
    "        l_token = df.tokens[index][n_tokens*last:]\n",
    "        if len(l_token) >0:\n",
    "          l_label = df.labels[index][n_tokens*last:]\n",
    "          l_docnum = str(df.document[index])+'|'+str(last)\n",
    "          sub = {\"document\":l_docnum,\"tokens\":l_token,\"labels\":l_label}\n",
    "          df_chunked = df_chunked._append(sub,ignore_index=True)\n",
    "    return df_chunked\n",
    "def changeFormat (df):\n",
    "    df_sub = chunkdf(df.loc[:,[\"document\",\"tokens\",\"labels\"]])\n",
    "    df_sub.labels=df_sub.labels.apply(lambda r : [labeldict[e] for e in r])\n",
    "\n",
    "    l=['O']\n",
    "    l.extend(full_ner_labels)\n",
    "    cl=ClassLabel(names=l)\n",
    "    # Sequence(feature=cl)\n",
    "    # d_train = Dataset.from_pandas(df_sub.loc[:,\"labels\"],features=Features({\"labels\":Sequence(feature=cl),\"__index_level_0__\":Value(\"string\")}))\n",
    "    d_train = Dataset.from_pandas(df_sub.loc[:,[\"labels\"]],features=Features({\"labels\":Sequence(feature=cl)}))\n",
    "    # d_train = Dataset.from_pandas(train.loc[:,[\"labels\"]])\n",
    "    # d_train=d_train.remove_columns('__index_level_0__')\n",
    "    d_train=d_train.rename_column(\"labels\",\"ner_tags\")\n",
    "    d_train=d_train.add_column('id',[str(s) for s in df_sub.document])\n",
    "    d_train=d_train.add_column('tokens',df_sub.tokens)\n",
    "    return d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:54:01.009207Z",
     "iopub.status.busy": "2024-02-28T13:54:01.008721Z",
     "iopub.status.idle": "2024-02-28T13:54:19.677839Z",
     "shell.execute_reply": "2024-02-28T13:54:19.676259Z",
     "shell.execute_reply.started": "2024-02-28T13:54:01.009172Z"
    }
   },
   "outputs": [],
   "source": [
    "df=train\n",
    "df_sub = chunkdf(df.loc[:,[\"document\",\"tokens\",\"labels\"]])\n",
    "#len(df.tokens[4879])\n",
    "# df_sub.loc[198,:]\n",
    "# len(df[df.document==21272].tokens)\n",
    "# (df[df.document==21272].tokens)\n",
    "# len(df.loc[6160,:].tokens)\n",
    "#len(df[df.document==21272].tokens)\n",
    "# for index,row in df_sub.iterrows():\n",
    "#   #print(len(df_sub.tokens[index]))\n",
    "#   assert len(df_sub.tokens[index])>0,print(f\"Lower length limit failed for {index}\")\n",
    "#   assert len(df_sub.tokens[index])<=400,print(f\"Upper length limit failed for {index}\")\n",
    "#   assert len(df_sub.tokens[index]) == len(df_sub.labels[index]),print(f\"Same length assertion failed for {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:57:12.388160Z",
     "iopub.status.busy": "2024-02-28T13:57:12.387685Z",
     "iopub.status.idle": "2024-02-28T13:57:14.163345Z",
     "shell.execute_reply": "2024-02-28T13:57:14.161852Z",
     "shell.execute_reply.started": "2024-02-28T13:57:12.388123Z"
    }
   },
   "outputs": [],
   "source": [
    "for index,row in df_sub.iterrows():\n",
    "    if(len(df_sub.tokens[index])==0 or len(df_sub.tokens[index])>400 or (len(df_sub.labels[index])!=len(df_sub.tokens[index]))):\n",
    "        df_sub = df_sub.drop(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:57:21.630202Z",
     "iopub.status.busy": "2024-02-28T13:57:21.629697Z",
     "iopub.status.idle": "2024-02-28T13:57:23.359316Z",
     "shell.execute_reply": "2024-02-28T13:57:23.358045Z",
     "shell.execute_reply.started": "2024-02-28T13:57:21.630166Z"
    }
   },
   "outputs": [],
   "source": [
    "for index,row in df_sub.iterrows():\n",
    "  #print(len(df_sub.tokens[index]))\n",
    "  assert len(df_sub.tokens[index])>0,print(f\"Lower length limit failed for {index}\")\n",
    "  assert len(df_sub.tokens[index])<=400,print(f\"Upper length limit failed for {index}\")\n",
    "  assert len(df_sub.tokens[index]) == len(df_sub.labels[index]),print(f\"Same length assertion failed for {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:57:26.354675Z",
     "iopub.status.busy": "2024-02-28T13:57:26.353824Z",
     "iopub.status.idle": "2024-02-28T13:58:25.725528Z",
     "shell.execute_reply": "2024-02-28T13:58:25.724184Z",
     "shell.execute_reply.started": "2024-02-28T13:57:26.354632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1291: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1317: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1291: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1317: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1291: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/table.py:1317: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "col_names = ['id', 'tokens', 'ner_tags']\n",
    "wnut_bert = DatasetDict()\n",
    "wnut_bert[\"train\"] = changeFormat(train)\n",
    "wnut_bert[\"test\"] = changeFormat(test)\n",
    "wnut_bert[\"validation\"] = changeFormat(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:58:50.063022Z",
     "iopub.status.busy": "2024-02-28T13:58:50.062510Z",
     "iopub.status.idle": "2024-02-28T13:58:50.295518Z",
     "shell.execute_reply": "2024-02-28T13:58:50.293583Z",
     "shell.execute_reply.started": "2024-02-28T13:58:50.062985Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:59:00.568335Z",
     "iopub.status.busy": "2024-02-28T13:59:00.567868Z",
     "iopub.status.idle": "2024-02-28T13:59:00.578923Z",
     "shell.execute_reply": "2024-02-28T13:59:00.577121Z",
     "shell.execute_reply.started": "2024-02-28T13:59:00.568301Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:59:03.312283Z",
     "iopub.status.busy": "2024-02-28T13:59:03.311801Z",
     "iopub.status.idle": "2024-02-28T13:59:39.852487Z",
     "shell.execute_reply": "2024-02-28T13:59:39.850900Z",
     "shell.execute_reply.started": "2024-02-28T13:59:03.312248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7345a4e017f94f25aef736e5fbd3c208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac4d752dacd4843b9ecc8f79f84d84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a656edf98c99480f9535cb9f146fcf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wnut = wnut_bert.map(tokenize_and_align_labels,batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:00:08.510036Z",
     "iopub.status.busy": "2024-02-28T14:00:08.508831Z",
     "iopub.status.idle": "2024-02-28T14:00:22.764438Z",
     "shell.execute_reply": "2024-02-28T14:00:22.763109Z",
     "shell.execute_reply.started": "2024-02-28T14:00:08.509987Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Pytorch\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:00:34.852047Z",
     "iopub.status.busy": "2024-02-28T14:00:34.851576Z",
     "iopub.status.idle": "2024-02-28T14:01:12.805869Z",
     "shell.execute_reply": "2024-02-28T14:01:12.804088Z",
     "shell.execute_reply.started": "2024-02-28T14:00:34.852005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=d28303f16263fd943c81338b884e144bf847fa28caddb598109094fb2edc9164\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:01:49.811647Z",
     "iopub.status.busy": "2024-02-28T14:01:49.811174Z",
     "iopub.status.idle": "2024-02-28T14:01:54.702803Z",
     "shell.execute_reply": "2024-02-28T14:01:54.701285Z",
     "shell.execute_reply.started": "2024-02-28T14:01:49.811591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa7b11d9edc4b59a4ec74103ef66c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:01:56.711130Z",
     "iopub.status.busy": "2024-02-28T14:01:56.710598Z",
     "iopub.status.idle": "2024-02-28T14:01:56.722148Z",
     "shell.execute_reply": "2024-02-28T14:01:56.720274Z",
     "shell.execute_reply.started": "2024-02-28T14:01:56.711089Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#example = wnut_bert[\"train\"][0]\n",
    "#labels = [full_ner_labels[i] for i in example[f\"ner_tags\"]]\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:01:59.695018Z",
     "iopub.status.busy": "2024-02-28T14:01:59.694507Z",
     "iopub.status.idle": "2024-02-28T14:01:59.703305Z",
     "shell.execute_reply": "2024-02-28T14:01:59.702242Z",
     "shell.execute_reply.started": "2024-02-28T14:01:59.694981Z"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-NAME_STUDENT\",\n",
    "    2: \"I-NAME_STUDENT\",\n",
    "    3: \"B-URL_PERSONAL\",\n",
    "    4: \"I-URL_PERSONAL\",\n",
    "    5: \"B-ID_NUM\",\n",
    "    6: \"I-ID_NUM\",\n",
    "    7: \"B-EMAIL\",\n",
    "    8: \"I-EMAIL\",\n",
    "    9: \"B-STREET_ADDRESS\",\n",
    "    10: \"I-STREET_ADDRESS\",\n",
    "    11: \"B-PHONE_NUM\",\n",
    "    12: \"I-PHONE_NUM\",\n",
    "    13: \"B-USERNAME\",\n",
    "    14: \"I-USERNAME\"\n",
    "}\n",
    "label2id = {\n",
    "    \"O\":0,\n",
    "    \"B-NAME_STUDENT\":1,\n",
    "    \"I-NAME_STUDENT\":2,\n",
    "    \"B-URL_PERSONAL\":3,\n",
    "    \"I-URL_PERSONAL\":4,\n",
    "    \"B-ID_NUM\":5,\n",
    "    \"I-ID_NUM\":6,\n",
    "    \"B-EMAIL\":7,\n",
    "    \"I-EMAIL\":8,\n",
    "    \"B-STREET_ADDRESS\":9,\n",
    "    \"I-STREET_ADDRESS\":10,\n",
    "    \"B-PHONE_NUM\":11,\n",
    "    \"I-PHONE_NUM\":12,\n",
    "    \"B-USERNAME\":13,\n",
    "    \"I-USERNAME\":14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:02:02.962252Z",
     "iopub.status.busy": "2024-02-28T14:02:02.961842Z",
     "iopub.status.idle": "2024-02-28T14:02:04.644868Z",
     "shell.execute_reply": "2024-02-28T14:02:04.643861Z",
     "shell.execute_reply.started": "2024-02-28T14:02:02.962220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191384fe555f47bfb0ec95150e88fc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# -- Pytorch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=15, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]\n",
    "! pip install -U accelerate\n",
    "# restart kernel after this installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:02:08.012538Z",
     "iopub.status.busy": "2024-02-28T14:02:08.012060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240228_140302-7nhu1t3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/f20220770/huggingface/runs/7nhu1t3h' target=\"_blank\">lunar-deluge-5</a></strong> to <a href='https://wandb.ai/f20220770/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/f20220770/huggingface' target=\"_blank\">https://wandb.ai/f20220770/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/f20220770/huggingface/runs/7nhu1t3h' target=\"_blank\">https://wandb.ai/f20220770/huggingface/runs/7nhu1t3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='1502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  71/1502 16:30 < 5:42:22, 0.07 it/s, Epoch 0.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Pytorch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
